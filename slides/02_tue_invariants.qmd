---
title: "Loop Invariants"
subtitle: "Week 2, Tuesday (Video)"
date: 2026-01-13
---

## From Monday

We now have two pillars:

1. **Efficiency** — How fast? (Big-$O$)
2. **Correctness** — Does it work?

## What Big-$O$ Actually Tells Us

Consider $T(n) = 3n^2 + 5n + 7$. What happens when $n$ doubles?

```{pyodide}
def T(n):
    return 3*n**2 + 5*n + 7

for n in [100, 200, 400, 800, 1600]:
    print(f"n={n:4}:  T(n)={T(n):>10,}   T(2n)/T(n)={T(2*n)/T(n):.2f}")
```

:::{.notes}
The ratio approaches 4. Always. For ANY quadratic.

"The constant 3, the +5n, the +7 — none of them matter. Doubling n → quadrupling T(n). That's what $O(n^2)$ means."
:::

## The Doubling Test

| Class | When $n$ doubles, time... |
|-------|---------------------------|
| $O(1)$ | stays the same |
| $O(\log n)$ | increases by a constant |
| $O(n)$ | doubles |
| $O(n \log n)$ | slightly more than doubles |
| $O(n^2)$ | quadruples |
| $O(2^n)$ | squares (!!) |

:::{.notes}
"This is what Big-O tells us: the SHAPE of growth, not the exact numbers."
:::

# The Practical Classes

## $O(1)$: Constant

```python
arr[0]
d[key]
stack.pop()
```

"No matter how big the input, same time."

:::{.notes}
Dictionary lookup, array access, stack operations.
:::

## $O(\log n)$: Logarithmic

```python
def binary_search(arr, target):
    lo, hi = 0, len(arr) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            lo = mid + 1
        else:
            hi = mid - 1
    return -1
```

:::{.notes}
"Halving at each step. $\log_2(1 \text{ billion}) \approx 30$."

"Worst case: target not in array. We halve until the range is empty → $\Theta(\log n)$."

We'll prove this correct on Wednesday.
:::

## $O(n)$: Linear

```python
def find_max(arr):
    max_val = arr[0]
    for x in arr:
        if x > max_val:
            max_val = x
    return max_val
```

:::{.notes}
"Must look at every element once."

"Worst case = every case: we always scan the whole array. $\Theta(n)$, tight."
:::

## $O(n \log n)$: Linearithmic

```python
sorted(arr)          # Python's built-in
df.sort_values()     # pandas
```

:::{.notes}
"The sweet spot for sorting. We'll see why in Week 6."
:::

## $O(n^2)$: Quadratic


::: {.columns}
::: {.column width="50%"}

```python
def has_duplicate(arr):
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False
```

:::

::: {.column width="50%"}
```{python}
#| echo: false
#| fig-align: center
import matplotlib.pyplot as plt
import matplotlib.patches as patches

fig, ax = plt.subplots(figsize=(4, 4))
n = 8
for i in range(n):
    for j in range(n):
        rect = patches.Rectangle((j, n-1-i), 1, 1, linewidth=1,
                                   edgecolor='black', facecolor='#3498db')
        ax.add_patch(rect)

ax.set_xlim(0, n)
ax.set_ylim(0, n)
ax.set_aspect('equal')
ax.axis('off')
plt.tight_layout()
plt.show()
```
:::
:::

:::{.notes}

why not j in range(n)?
"All pairs. Gets painful quickly."

"Worst case: no duplicates → check all $n(n-1)/2$ pairs. $\Theta(n^2)$, tight."
:::

## $O(2^n)$: Exponential

```python
def all_subsets(arr):
    if not arr:
        return [[]]
    rest = all_subsets(arr[1:])
    return rest + [[arr[0]] + s for s in rest]
```

```{pyodide}
for n in [10, 20, 30, 40, 50, 100]:
    print(f"n={n:3}:  2ⁿ = {2**n:,}")
```

:::{.notes}
"Doubles with each element."

n=50: more operations than atoms in a grain of sand.
n=100: more operations than atoms in the observable universe.

At 1 billion ops/sec, n=100 would take $10^{21}$ years — heat death of universe.

"Often intractable. We'll see NP-completeness in Week 12."
:::

# Racing Algorithms

## The Setup

```{pyodide}
import time

def has_dup_quadratic(arr):
    for i in range(len(arr)):
        for j in range(i+1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False

def has_dup_linear(arr):
    seen = set()
    for x in arr:
        if x in seen:
            return True
        seen.add(x)
    return False
```

:::{.notes}
"Same problem, two approaches. Let's race them."
:::

## The Race

```{pyodide}
def time_it(func, arr):
    start = time.time()
    func(arr)
    return time.time() - start

print("Size     Θ(n²)       Θ(n)")
print("-" * 35)
for n in [1000, 2000, 4000, 8000]:
    arr = list(range(n))  # No duplicates = worst case for both
    t1 = time_it(has_dup_quadratic, arr)
    t2 = time_it(has_dup_linear, arr)
    print(f"{n:5}   {t1:.4f}s   {t2:.5f}s")
```

:::{.notes}
"We're testing worst case: no duplicates, so both algorithms scan everything."

"What do you notice?"

$\Theta(n^2)$: 4× slower when n doubles — that's quadratic!
$\Theta(n)$: barely changes — that's linear!

"The set gives $O(1)$ lookup — transforms $\Theta(n^2)$ into $\Theta(n)$."
:::

## What Happened

Worst case: $\Theta(n^2) \to \Theta(n)$

The **set** provides $O(1)$ lookup.

:::{.notes}
"This is the power of data structures. We'll learn how sets work in Week 5."

"Both bounds are tight. We're not hedging — we analyzed the worst case and know exactly how it scales."
:::

# When Does It Matter?

## The Crossover

```{pyodide}
def slow_constant(n):
    time.sleep(0.01)  # 10ms setup
    return 42

def fast_linear(n):
    total = 0
    for i in range(n):
        total += i
    return total

for n in [10, 100, 1000]:
    t1 = time_it(slow_constant, n)
    t2 = time_it(fast_linear, n)
    print(f"n={n:4}:  O(1)={t1:.3f}s   O(n)={t2:.5f}s")
```

:::{.notes}
"At small n, constants dominate. The '$O(1)$' algorithm with 10ms overhead is slower!"

Live demo: add larger n values (10000, 100000, 1000000) to show the crossover where linear catches up and passes.
:::

## The Lesson

Small data: write clear code.

Big data: **Big-O is destiny.**

## Galactic Pizza

```{pyodide}
import pandas as pd
import numpy as np
import time

n = 500_000
np.random.seed(42)

df = pd.DataFrame({
    'delivery_time': np.random.exponential(scale=30, size=n),
    'planet': np.random.choice(['Earth', 'Mars', 'Europa', 'Titan'], n),
})

print(f"Rows: {len(df):,}")
print(df.head(8))
```

## Two Ways to Compute the Mean

```{pyodide}
def iter_mean(df, col):
    total = 0
    for idx, row in df.iterrows():
        total += row[col]
    return total / len(df)

start = time.time()
avg1 = iter_mean(df, 'delivery_time')
t1 = time.time() - start

start = time.time()
avg2 = df['delivery_time'].mean()
t2 = time.time() - start

print(f"Loop:       {t1:.3f} sec")
print(f"Vectorized: {t2:.5f} sec")
print(f"Speedup:    {t1/t2:,.0f}×")
```

:::{.notes}
"Both are $O(n)$. Why the difference?"
:::

## Same Big-$O$, Different Speed

Both touch every row: $O(n)$

But:

- **Vectorized**: tiny constants (compiled C)
- **Python loop**: large constants (interpreter overhead)

:::{.notes}
"Big-O tells you SCALING, not SPEED."

"In pandas: prefer vectorized operations. But that's about constants, not complexity class."
:::

# The Pillar of Correctness

## Trusting Your Code

```python
def ________________(arr, a):
    ret = a
    for i in range(a + 1, len(arr)):
        if arr[i] < arr[ret]:
            ret = i
    return ret
```

- What does this function do?
- Does it work?
- How do you **know**?

:::{.notes}
"You might say 'I tested it.' But testing checks examples, not all inputs."

"We want mathematical certainty."
:::

## Testing Is Not Proof

Testing: "It worked on these 10 inputs."

Proof: "It works on **all** valid inputs."

:::{.notes}
"Dijkstra: 'Testing shows the presence of bugs, not their absence.'"
:::

## The Key Insight

A loop is just **induction** in disguise.

| Induction | Loop Invariant |
|-----------|---------------|
| Base case | Initialization |
| Inductive step | Maintenance |
| "For all $n$" | Termination |

:::{.notes}
"You already know induction from discrete math. Loop invariants are the same idea applied to code."
:::

# The Template

## What Is an Invariant?

An **invariant** is a property that remains true.

"No matter how many times the loop runs, this statement stays true."

:::{.notes}
"The word 'invariant' = 'not varying' = stays the same."
:::

## The Template

| Step | What to show |
|------|--------------|
| **Invariant** | State $P(i)$ for $i \in \{0, 1, \ldots, n\}$ (your domain) |
| **Base case** | $P(0)$ holds (first value in domain) |
| **Inductive step** | Assuming $P(j)$ for all $j$ in domain where $j < i$, show $P(i)$ |
| **Termination** | Plug in $i = n$ → desired result |

:::{.notes}
"This is just induction on the iteration count. The domain of i depends on your loop — it might be $\{0, \ldots, n\}$ or $\{a+1, \ldots, n\}$ or something else."
:::

# Example: Find Min

## The Code

```python
def find_min(arr, a):
    """Return index of minimum value in arr[a:]"""
    ret = a
    for i in range(a + 1, len(arr)):
        if arr[i] < arr[ret]:
            ret = i
    return ret
```

| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| 32 | 11 | 73 | 21 | 29 | 86 | 81 | 17 |

If `a = 2`, what should `find_min` return?

:::{.notes}
"Looking at arr[2:] = [73, 21, 29, 86, 81, 7], the minimum is 7 at index 7."

"Let's prove this function always returns the correct answer."
:::

## Step 1: State the Invariant

```python
def find_min(arr, a):
    """Return index of minimum value in arr[a:]"""
    ret = a
    for i in range(a + 1, len(arr)):
        if arr[i] < arr[ret]:
            ret = i
    return ret
```

| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| 32 | 11 | 73 | 21 | 29 | 86 | 81 | 17 | 14 | 3 | 64 | 37 |

**Invariant**: At iteration $i$ (for $i \in \{a+1, \ldots, n\}$), `ret` holds the index of the minimum value in `arr[a:i]`.

:::{.notes}
"The hard part is finding the right invariant. Once you have it, the proof often falls into place."

"This invariant captures what we're maintaining as we go: ret is the min of everything we've seen so far."

"Note: i ranges from a+1 to n."
:::

## Step 2: Base Case ($i = a+1$)

```python
def find_min(arr, a):
    """Return index of minimum value in arr[a:]"""
    ret = a
    for i in range(a + 1, len(arr)):
        if arr[i] < arr[ret]:
            ret = i
    return ret
```

At the first iteration, `i = a+1` and `ret = a`.

The invariant claims `ret` is the index of the min in `arr[a:a+1]`.

`arr[a:a+1]` contains only `arr[a]`, and `ret = a` is its index. ✓

:::{.notes}
"This is the base case. We've 'seen' only arr[a], and ret points to it."
:::

## Step 3: Inductive Step {.smaller}

```python
def find_min(arr, a):
    """Return index of minimum value in arr[a:]"""
    ret = a
    for i in range(a + 1, len(arr)):
        if arr[i] < arr[ret]:
            ret = i
    return ret
```
| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| 32 | 11 | 73 | 21 | 29 | 86 | 81 | 17 | 14 | 3 | 64 | 37 |

**IH**: Assume the invariant holds at the start of iteration $i$: `ret` is the index of min in `arr[a:i]`.

During iteration $i$, we compare `arr[i]` to `arr[ret]`:

- **Case 1**: `arr[i] < arr[ret]` → we set `ret = i`, the new min
- **Case 2**: `arr[i] >= arr[ret]` → we keep `ret`, still the min

Either way, after iteration $i$, `ret` is the index of min in `arr[a:i+1]`. ✓

:::{.notes}
"The two cases correspond to the if/else. In both cases, ret ends up pointing to the minimum of arr[a:i+1], which is the invariant for iteration i+1."
:::

## Step 4: Termination

Loop terminates when `i = n` (after processing the last element).

Plug into invariant: `ret` is index of min in `arr[a:n]`.

But `arr[a:n]` is exactly `arr[a:]`.

**Therefore**: `ret` is the index of the minimum in `arr[a:]`. ∎

:::{.notes}
"At i = n, ret is min of arr[a:n] = arr[a:]."

"The ∎ (tombstone/halmos) marks the end of a proof."
:::

## The Proof in One Slide

**Theorem**: `find_min(arr, a)` returns the index of the minimum value in `arr[a:]`.

**Proof**: By induction on iteration count.

- **Invariant**: At iteration $i$ (for $i \in \{a+1, \ldots, n\}$), `ret` is index of min in `arr[a:i]`
- **Base case**: At $i = a+1$, `ret = a` is min of `arr[a:a+1]` ✓
- **Inductive step**: If true at start of iteration $i$, loop body restores it for $i+1$ ✓
- **Termination**: Plug in $i = n$ → `ret` is min of `arr[a:n]` = `arr[a:]` ∎

# Another Example: Sum

## The Code

```python
def array_sum(arr):
    total = 0
    for i in range(len(arr)):
        total += arr[i]
    return total
```

What's the invariant?

:::{.notes}
"Pause the video and try to state the invariant before I reveal it."
:::

## The Invariant

**Invariant**: At iteration $i$ (for $i \in \{0, 1, \ldots, n\}$), `total` equals the sum of `arr[0:i]`.

:::{.notes}
"This captures what we're building up as we go. Here i ranges from 0 to n."
:::

## The Proof (Quick Version)

1. **Base case**: At $i = 0$, `total = 0` = sum of `arr[0:0]`. ✓

2. **Inductive step**: Assume invariant holds at start of iteration $i$: `total` = sum of `arr[0:i]`.

   During iteration $i$, we add `arr[i]` to `total`.

   After iteration $i$: `total` = sum of `arr[0:i]` + `arr[i]` = sum of `arr[0:i+1]`. ✓

3. **Termination**: Plug in $i = n$ → `total` = sum of `arr[0:n]` = sum of all. ∎

:::{.notes}
"With practice, these proofs become quick sketches, not page-long arguments."
:::

# Composing Proofs

## Selection Sort

```python
def selection_sort(arr):
    for a in range(len(arr)):
        min_idx = find_min(arr, a)
        arr[a], arr[min_idx] = arr[min_idx], arr[a]
```

- This uses our `find_min` helper!

- Quick check: running time? 

:::{.notes}
"Selection sort: repeatedly find the minimum of the unsorted portion and swap it to the front."

"Notice it calls `find_min(arr, a)` — exactly the function we just proved correct."
:::

## The Decomposition Principle

We already proved `find_min(arr, a)` returns the index of the minimum in `arr[a:]`.

Now we can **use that fact** without re-proving it.

> Prove small pieces. Compose them into bigger proofs.

:::{.notes}
"This is the software engineering principle of decomposition, applied to proofs."

"You don't re-derive addition every time you use it. Same idea here."
:::

## The Selection Sort Invariant

**Invariant**: At iteration $a$ (for $a \in \{0, 1, \ldots, n\}$), `arr[0:a]` is sorted and contains the $a$ smallest elements.

:::{.notes}
"The first a elements are sorted AND they're the smallest a elements overall."

"Both parts matter — sorted isn't enough; they have to be the right elements."
:::

## The Proof {.smaller}

1. **Base case**: At $a = 0$, `arr[0:0]` is empty. ✓ (Trivially sorted, trivially smallest.)

2. **Inductive step**: Assume invariant holds at start of iteration $a$: `arr[0:a]` is sorted and contains the $a$ smallest elements.

   During iteration $a$:

   - `find_min(arr, a)` returns the index of the minimum in `arr[a:]` *(by our earlier proof!)*
   - After the swap, `arr[a]` holds the $(a+1)$-th smallest element

   After iteration $a$: `arr[0:a+1]` is sorted and contains the $a+1$ smallest elements. ✓

3. **Termination**: Plug in $a = n$ → `arr[0:n]` is sorted. ∎

:::{.notes}
"See how we just *used* the find_min correctness? We didn't re-prove it."

"This is the power of decomposition: prove once, use everywhere."
:::

## The Lesson

**Decomposition** is good *system* engineering.

| Software | Proofs | Data Pipelines |
|----------|--------|----------------|
| Write small, tested functions | Prove small helper lemmas | Build validated transformations |
| Compose into larger programs | Compose into larger proofs | Chain into workflows |
| Reuse without re-implementing | Reuse without re-proving | Reuse without re-validating |

:::{.notes}
"When you write `find_min` as a separate function, you're setting yourself up for cleaner code AND cleaner proofs."

"Same for data science: build a cleaning step you trust, a feature extraction step you trust, then compose them. You don't re-check the cleaning every time you use it."
:::

# Finding the Invariant

## The Art

Stating the invariant is often the hardest part.

**Strategy**: Ask "What property do I maintain that leads to my answer?"

:::{.notes}
"The invariant should connect the loop's progress to the final goal."
:::

## Common Patterns

| Algorithm Type | Invariant Pattern |
|---------------|-------------------|
| Accumulation | "`var` holds result for elements seen so far" |
| Search | "If target exists, it's in the remaining range" |
| Partition | "Elements before $i$ satisfy property P" |
| Two-pointer | "Answer is not in the excluded region" |

:::{.notes}
"Wednesday: we'll use the 'search' pattern for binary search."
:::

## Design, Not Just Proof

Loop invariants aren't just for proving code correct.

They're a **design tool**.

> "What should be true at each step?"

If you can't state the invariant, you might not understand your own algorithm.

:::{.notes}
"When you're stuck debugging, ask: what's my invariant? Often the bug is that you're not maintaining it."
:::

## A Warning

A **wrong** invariant can "prove" incorrect code correct.

The invariant must be:

1. **True** (actually holds at each iteration)
2. **Strong enough** (implies correctness at termination)
3. **Preserved** (maintained by the loop body)

:::{.notes}
"You can't just make up any statement. It has to capture the actual behavior."
:::

## A Bad Example {.smaller}

**Weak invariant**: "At iteration $a$, `arr[0:a]` is sorted."

| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| 12 | 25 | 38 | 47 | 59 | 83 | 71 | 6 | 94 | 52 | 15 | 33 |

At iteration $a = 5$, suppose we have:

| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| 12 | 25 | 38 | 47 | 59 | 83 | 71 | 6 | 94 | 52 | 15 | 33 |

Is `arr[0:5]` = `[12, 25, 38, 47, 59]` sorted? **Yes!**

But is this correct? **No!** The element 6 is smaller than all of them.

:::{.notes}
"The invariant is TRUE but not STRONG ENOUGH. The first 5 elements happen to be in order, but they're not the 5 smallest. We need: 'arr[0:a] is sorted AND contains the a smallest elements.'"
:::

# Why This Matters

## For Your Career

Real bugs in real systems come from:

- Off-by-one errors
- Edge cases not handled
- "It usually works" code

Loop invariants catch these **before** they ship.

:::{.notes}
"Thinking in invariants makes you a better programmer, even if you don't write formal proofs."
:::

## The Mindset

Every time you write a loop, ask:

1. What's true at the start?
2. What stays true each iteration?
3. What does that give me at the end?

:::{.notes}
"Make this a habit. It prevents bugs."
:::

# Summary

## The Loop Invariant Template

| Step | What to show |
|------|--------------|
| **Invariant** | State $P(i)$ for $i \in \{a, a+1, \ldots, n\}$ (your domain) |
| **Base case** | $P(a)$ holds (first value in domain) |
| **Inductive step** | Assuming $P(j)$ for all $a \leq j < i$, show $P(i)$ |
| **Termination** | Plug in $i = n$ → desired result |

This is **induction** on the iteration count.

## Looking Ahead

**Wednesday**: We'll use loop invariants to prove a famous algorithm correct — and see why it took 17 years to get a bug-free version.

## Practice Problems

Before Wednesday, try stating invariants for:

1. `reverse(arr)` — reverse an array in place

2. `is_sorted(arr)` — check if array is sorted

3. `count(arr, target)` — count occurrences of target

Think: What's true after processing $i$ elements?
