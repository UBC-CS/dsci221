---
title: "Loop Invariants"
subtitle: "Week 2, Tuesday (Video)"
date: 2026-01-13
---

## From Monday

We now have two pillars:

1. **Efficiency** — How fast? (Big-$O$)
2. **Correctness** — Does it work?

## What Big-$O$ Actually Tells Us

Consider $T(n) = 3n^2 + 5n + 7$. What happens when $n$ doubles?

```{pyodide}
def T(n):
    return 3*n**2 + 5*n + 7

for n in [100, 200, 400, 800, 1600]:
    print(f"n={n:4}:  T(n)={T(n):>10,}   T(2n)/T(n)={T(2*n)/T(n):.2f}")
```

:::{.notes}
The ratio approaches 4. Always. For ANY quadratic.

"The constant 3, the +5n, the +7 — none of them matter. Doubling n → quadrupling T(n). That's what $O(n^2)$ means."
:::

## The Doubling Test

| Class | When $n$ doubles, time... |
|-------|---------------------------|
| $O(1)$ | stays the same |
| $O(\log n)$ | increases by a constant |
| $O(n)$ | doubles |
| $O(n \log n)$ | slightly more than doubles |
| $O(n^2)$ | quadruples |
| $O(2^n)$ | squares (!!) |

:::{.notes}
"This is what Big-O tells us: the SHAPE of growth, not the exact numbers."
:::

# The Practical Classes

## $O(1)$: Constant

```python
arr[0]
d[key]
stack.pop()
```

"No matter how big the input, same time."

:::{.notes}
Dictionary lookup, array access, stack operations.
:::

## $O(\log n)$: Logarithmic

```python
def binary_search(arr, target):
    lo, hi = 0, len(arr) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            lo = mid + 1
        else:
            hi = mid - 1
    return -1
```

:::{.notes}
"Halving at each step. $\log_2(1 \text{ billion}) \approx 30$."

"Worst case: target not in array. We halve until the range is empty → $\Theta(\log n)$."

We'll prove this correct on Wednesday.
:::

## $O(n)$: Linear

```python
def find_max(arr):
    max_val = arr[0]
    for x in arr:
        if x > max_val:
            max_val = x
    return max_val
```

:::{.notes}
"Must look at every element once."

"Worst case = every case: we always scan the whole array. $\Theta(n)$, tight."
:::

## $O(n \log n)$: Linearithmic

```python
sorted(arr)          # Python's built-in
df.sort_values()     # pandas
```

:::{.notes}
"The sweet spot for sorting. We'll see why in Week 6."
:::

## $O(n^2)$: Quadratic


::: {.columns}
::: {.column width="50%"}

```python
def has_duplicate(arr):
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False
```

:::

::: {.column width="50%"}
```{python}
#| echo: false
#| fig-align: center
import matplotlib.pyplot as plt
import matplotlib.patches as patches

fig, ax = plt.subplots(figsize=(4, 4))
n = 8
for i in range(n):
    for j in range(n):
        rect = patches.Rectangle((j, n-1-i), 1, 1, linewidth=1,
                                   edgecolor='black', facecolor='#3498db')
        ax.add_patch(rect)

ax.set_xlim(0, n)
ax.set_ylim(0, n)
ax.set_aspect('equal')
ax.axis('off')
plt.tight_layout()
plt.show()
```
:::
:::

:::{.notes}

why not j in range(n)?
"All pairs. Gets painful quickly."

"Worst case: no duplicates → check all $n(n-1)/2$ pairs. $\Theta(n^2)$, tight."
:::

## $O(2^n)$: Exponential

```python
def all_subsets(arr):
    if not arr:
        return [[]]
    rest = all_subsets(arr[1:])
    return rest + [[arr[0]] + s for s in rest]
```

```{pyodide}
for n in [10, 20, 30, 40, 50, 100]:
    print(f"n={n:3}:  2ⁿ = {2**n:,}")
```

:::{.notes}
"Doubles with each element."

n=50: more operations than atoms in a grain of sand.
n=100: more operations than atoms in the observable universe.

At 1 billion ops/sec, n=100 would take $10^{21}$ years — heat death of universe.

"Often intractable. We'll see NP-completeness in Week 12."
:::

# Racing Algorithms

## The Setup

```{pyodide}
import time

def has_dup_quadratic(arr):
    for i in range(len(arr)):
        for j in range(i+1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False

def has_dup_linear(arr):
    seen = set()
    for x in arr:
        if x in seen:
            return True
        seen.add(x)
    return False
```

:::{.notes}
"Same problem, two approaches. Let's race them."
:::

## The Race

```{pyodide}
def time_it(func, arr):
    start = time.time()
    func(arr)
    return time.time() - start

print("Size     Θ(n²)       Θ(n)")
print("-" * 35)
for n in [1000, 2000, 4000, 8000]:
    arr = list(range(n))  # No duplicates = worst case for both
    t1 = time_it(has_dup_quadratic, arr)
    t2 = time_it(has_dup_linear, arr)
    print(f"{n:5}   {t1:.4f}s   {t2:.5f}s")
```

:::{.notes}
"We're testing worst case: no duplicates, so both algorithms scan everything."

"What do you notice?"

$\Theta(n^2)$: 4× slower when n doubles — that's quadratic!
$\Theta(n)$: barely changes — that's linear!

"The set gives $O(1)$ lookup — transforms $\Theta(n^2)$ into $\Theta(n)$."
:::

## What Happened

Worst case: $\Theta(n^2) \to \Theta(n)$

The **set** provides $O(1)$ lookup.

:::{.notes}
"This is the power of data structures. We'll learn how sets work in Week 5."

"Both bounds are tight. We're not hedging — we analyzed the worst case and know exactly how it scales."
:::

# When Does It Matter?

## The Crossover

```{pyodide}
def slow_constant(n):
    time.sleep(0.01)  # 10ms setup
    return 42

def fast_linear(n):
    total = 0
    for i in range(n):
        total += i
    return total

for n in [10, 100, 1000]:
    t1 = time_it(slow_constant, n)
    t2 = time_it(fast_linear, n)
    print(f"n={n:4}:  O(1)={t1:.3f}s   O(n)={t2:.5f}s")
```

:::{.notes}
"At small n, constants dominate. The '$O(1)$' algorithm with 10ms overhead is slower!"

Live demo: add larger n values (10000, 100000, 1000000) to show the crossover where linear catches up and passes.
:::

## The Lesson

Small data: write clear code.

Big data: **Big-O is destiny.**

## Galactic Pizza

```{pyodide}
import pandas as pd
import numpy as np
import time

n = 500_000
np.random.seed(42)

df = pd.DataFrame({
    'delivery_time': np.random.exponential(scale=30, size=n),
    'planet': np.random.choice(['Earth', 'Mars', 'Europa', 'Titan'], n),
})

print(f"Rows: {len(df):,}")
print(df.head(8))
```

## Two Ways to Compute the Mean

```{pyodide}
def iter_mean(df, col):
    total = 0
    for idx, row in df.iterrows():
        total += row[col]
    return total / len(df)

start = time.time()
avg1 = iter_mean(df, 'delivery_time')
t1 = time.time() - start

start = time.time()
avg2 = df['delivery_time'].mean()
t2 = time.time() - start

print(f"Loop:       {t1:.3f} sec")
print(f"Vectorized: {t2:.5f} sec")
print(f"Speedup:    {t1/t2:,.0f}×")
```

:::{.notes}
"Both are $O(n)$. Why the difference?"
:::

## Same Big-$O$, Different Speed

Both touch every row: $O(n)$

But:

- **Vectorized**: tiny constants (compiled C)
- **Python loop**: large constants (interpreter overhead)

:::{.notes}
"Big-O tells you SCALING, not SPEED."

"In pandas: prefer vectorized operations. But that's about constants, not complexity class."
:::

