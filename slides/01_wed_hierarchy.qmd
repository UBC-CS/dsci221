---
title: "The Hierarchy of Speed"
subtitle: "Week 1, Wednesday"
date: 2026-01-07
---

# The Growth Rate Ladder

## From Tuesday

We learned to count operations and express them as Big-O.

. . .

Today: **Which Big-O classes matter, and why?**

## The hierarchy

$$O(1) \quad < \quad O(\log n) \quad < \quad O(n) \quad < \quad O(n \log n) \quad < \quad O(n^2) \quad < \quad O(2^n)$$

. . .

Let's see what these actually *mean* at scale.

## At n = 1,000,000

| Big-O | Operations | Time at 1 billion ops/sec |
|-------|------------|---------------------------|
| O(1) | 1 | 1 nanosecond |
| O(log n) | 20 | 20 nanoseconds |
| O(n) | 1,000,000 | 1 millisecond |
| O(n log n) | 20,000,000 | 20 milliseconds |
| O(n²) | 1,000,000,000,000 | 16 minutes |
| O(2ⁿ) | 10³⁰⁰'⁰⁰⁰ | heat death of universe |

## The cliff

On a log scale, even n² looks gentle. But 2ⁿ goes vertical.

```{pyodide}
import math

# Let's compute actual values for different n
for n in [10, 20, 30, 40]:
    ops_n2 = n**2
    ops_2n = 2**n
    print(f"n={n:2}: n²={ops_n2:>12,}    2ⁿ={ops_2n:>15,}")
```

# The Practical Classes

## O(1): Constant time

```{pyodide}
#| autorun: false
def get_first(arr):
    return arr[0]

def dict_lookup(d, key):
    return d[key]
```

. . .

**No matter how big the input, same time.**

- Dictionary/set lookup
- Array access by index
- Stack push/pop

## O(log n): Logarithmic time

```{pyodide}
#| autorun: false
def binary_search(arr, target):
    lo, hi = 0, len(arr) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            lo = mid + 1
        else:
            hi = mid - 1
    return -1
```

. . .

**Halving at each step.** Grows incredibly slowly.

log₂(1 billion) ≈ 30

## O(n): Linear time

```{pyodide}
#| autorun: false
def find_max(arr):
    max_val = arr[0]
    for x in arr:
        if x > max_val:
            max_val = x
    return max_val
```

. . .

**Must look at every element once.**

- Finding min/max
- Counting occurrences
- df['col'].sum()

## O(n log n): Linearithmic time

```{pyodide}
#| autorun: false
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    return merge(left, right)
```

. . .

**The sweet spot for sorting.**

- Merge sort, quicksort (average)
- `sorted()` in Python
- `df.sort_values()`

## O(n²): Quadratic time

```{pyodide}
#| autorun: false
def has_duplicate_naive(arr):
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False
```

. . .

**All pairs.** Gets painful quickly.

- Naive duplicate detection
- Bubble sort
- Some matrix operations

## O(2ⁿ): Exponential time

```{pyodide}
#| autorun: false
def all_subsets(arr):
    if not arr:
        return [[]]
    rest = all_subsets(arr[1:])
    return rest + [[arr[0]] + s for s in rest]
```

. . .

**Doubles with each element.** Often intractable.

- Generating all subsets
- Brute-force combinatorics
- Many NP-hard problems

# Feeling the Difference

## Let's race some algorithms

```{pyodide}
import time

def time_it(func, *args):
    start = time.time()
    result = func(*args)
    return time.time() - start

# O(n²) duplicate check
def has_dup_quadratic(arr):
    for i in range(len(arr)):
        for j in range(i+1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False

# O(n) duplicate check with set
def has_dup_linear(arr):
    seen = set()
    for x in arr:
        if x in seen:
            return True
        seen.add(x)
    return False
```

## The race

```{pyodide}
sizes = [500, 1000, 2000, 4000]
print("Size     O(n²)      O(n)     Ratio")
print("-" * 40)

for n in sizes:
    arr = list(range(n))  # No duplicates (worst case)

    t_quad = time_it(has_dup_quadratic, arr)
    t_lin = time_it(has_dup_linear, arr)

    if t_lin > 0:
        ratio = t_quad/t_lin
        print(f"{n:5}   {t_quad:.4f}s   {t_lin:.5f}s   {ratio:6.0f}x")
    else:
        print(f"{n:5}   {t_quad:.4f}s   {t_lin:.5f}s   (very fast)")
```

## What just happened?

::: {.columns}
::: {.column width="50%"}
**O(n²)**

- 1000 → 2000: ~4× slower
- 2000 → 4000: ~4× slower
- Grows fast!
:::

::: {.column width="50%"}
**O(n)**

- Time barely changes
- **Orders of magnitude faster**
:::
:::

. . .

The set gives us O(1) lookup → transforms O(n²) into O(n).

# When Does It Matter?

## The crossover point

At small n, constants dominate:

```{pyodide}
import time

def slow_constant(n):
    """O(1) but with large constant"""
    time.sleep(0.01)  # 10ms of "setup"
    return 42

def fast_linear(n):
    """O(n) but with tiny constant"""
    total = 0
    for i in range(n):
        total += i
    return total
```

## Small n: constants win

```{pyodide}
for n in [10, 100, 1000]:
    t1 = time_it(slow_constant, n)
    t2 = time_it(fast_linear, n)
    print(f"n={n:4}: O(1)={t1:.4f}s  O(n)={t2:.6f}s  Winner: {'O(1)' if t1 < t2 else 'O(n)'}")
```

. . .

At n=10, the "O(1)" algorithm is slower due to its 10ms constant!

## Large n: Big-O wins

```{pyodide}
for n in [10000, 100000, 1000000]:
    t1 = time_it(slow_constant, n)
    t2 = time_it(fast_linear, n)
    print(f"n={n:7}: O(1)={t1:.4f}s  O(n)={t2:.4f}s  Winner: {'O(1)' if t1 < t2 else 'O(n)'}")
```

. . .

At n=1,000,000, Big-O dominates. The linear algorithm is 100× slower.

## The lesson

::: {.callout-important}
## Big-O matters when n is large

For small data: just write clear code.

For big data: **Big-O is destiny.**

Know your data size. Choose accordingly.
:::

# Back to Shazam

## The three-act structure

::: {.incremental}
1. **Naive search**: Compare sample to every song → O(n) where n = 100 million
2. **Smarter search**: Sort songs somehow? Binary search? → O(log n)?
3. **Brilliant insight**: Convert audio to fingerprint, use hash table → O(1)
:::

## How it works (preview)

1. Convert audio to spectrogram (time × frequency)
2. Find **peaks** — these are robust to noise
3. Turn pairs of peaks into a **hash** (a number)
4. Look up that hash in a **hash table**

. . .

Hash table lookup is O(1) → **doesn't matter if database has 1 million or 100 million songs!**

## The magic formula

::: {.r-fit-text}
**Right data structure** + **Right algorithm**

= **Impossible becomes instant**
:::

. . .

This is what DSCI 221 is about.

# This Week's Lab

## Timing experiments

You'll:

1. Implement O(n) and O(n²) solutions to the same problem
2. Predict their running times at various sizes
3. Measure actual times
4. Explain any discrepancies

. . .

Science mode: hypothesis → experiment → analysis

## Predictions matter

::: {.callout-tip}
## The prediction game

Before you run code, **write down what you expect**.

If n doubles and your algorithm is O(n²), time should ~4×.

If your prediction is wrong, you learn something!
:::

# Summary

## The hierarchy

| Class | Name | Example |
|-------|------|---------|
| O(1) | Constant | Dictionary lookup |
| O(log n) | Logarithmic | Binary search |
| O(n) | Linear | Finding maximum |
| O(n log n) | Linearithmic | Sorting |
| O(n²) | Quadratic | Comparing all pairs |
| O(2ⁿ) | Exponential | All subsets |

## The takeaways

::: {.incremental}
1. **Big-O tells you how things scale**, not absolute speed
2. **At large n, Big-O dominates** — constants become irrelevant
3. **The right data structure can drop you down the hierarchy** (O(n²) → O(n))
4. **This is why data structures matter** — they enable faster algorithms
:::

## Next week

**Binary search**: the power of sorted data.

Can you guess a number from 1 to 1,000,000 in 20 questions?

. . .

::: {.r-fit-text}
**Spoiler**: Yes, and we'll prove it works.
:::
