---
title: "The Hierarchy of Speed"
subtitle: "Week 1, Wednesday"
date: 2026-01-07
---

# From Tuesday

## Patterns

| Pattern | When n doubles... |
|---------|-------------------|
| Single loop | Work doubles |
| Nested loops | Work quadruples |
| Halving | One more step |

:::{.notes}
"Today: formalize this, then FEEL the difference."
:::

# Formalizing Big-O

## The Definition

f(n) is **O(g(n))** if there exist constants c > 0 and n₀ ≥ 0 such that:

$$f(n) \leq c \cdot g(n) \quad \text{for all } n \geq n_0$$

:::{.notes}
"In words: f grows no faster than g, once n is big enough."
:::

## Picture

```{python}
#| echo: false
import matplotlib.pyplot as plt
import numpy as np

n = np.linspace(1, 20, 100)
f_n = 2*n + 3
g_n = 3*n

plt.figure(figsize=(8, 5))
plt.plot(n, f_n, 'b-', linewidth=2, label='f(n) = 2n + 3')
plt.plot(n, g_n, 'r--', linewidth=2, label='c·g(n) = 3n')
plt.axvline(x=3, color='green', linestyle=':', linewidth=2, label='n₀ = 3')
plt.fill_between(n[n >= 3], f_n[n >= 3], g_n[n >= 3], alpha=0.3, color='green')
plt.xlabel('n', fontsize=12)
plt.ylabel('Operations', fontsize=12)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

:::{.notes}
For n ≥ 3: 2n + 3 ≤ 3n

Choose c = 3, n₀ = 3.
:::

## Other Notations

| Notation | Meaning |
|----------|---------|
| O(g(n)) | ≤ c·g(n) — upper bound |
| Ω(g(n)) | ≥ c·g(n) — lower bound |
| Θ(g(n)) | both — tight bound |

:::{.notes}
"We mostly use O — the worst-case guarantee."
:::

## Prove It

Show that 3n² + 5n + 2 = O(n²)

:::{.notes}
Let them try.

For n ≥ 1:
- 3n² ≤ 3n²
- 5n ≤ 5n²
- 2 ≤ 2n²

So: 3n² + 5n + 2 ≤ 10n²

Choose c = 10, n₀ = 1. ✓
:::

## Rules

1. Constants don't matter: 5n = O(n)
2. Lower terms don't matter: n² + n = O(n²)
3. Log base doesn't matter: log₂n = O(log₁₀n)

:::{.notes}
"What's the intuition for #3?"

All logs differ by a constant factor. log₂n = log₁₀n / log₁₀2
:::

# The Hierarchy

## The Ladder

$$O(1) < O(\log n) < O(n) < O(n \log n) < O(n^2) < O(2^n)$$

:::{.notes}
"Let's see what these mean at scale."
:::

## At n = 1,000,000

| Big-O | Operations | Time @ 1B ops/sec |
|-------|------------|-------------------|
| O(1) | 1 | 1 nanosecond |
| O(log n) | 20 | 20 nanoseconds |
| O(n) | 1,000,000 | 1 millisecond |
| O(n log n) | 20,000,000 | 20 milliseconds |
| O(n²) | 10¹² | 16 minutes |
| O(2ⁿ) | 10³⁰⁰'⁰⁰⁰ | heat death of universe |

## The Cliff

```{pyodide}
for n in [10, 20, 30, 40, 50]:
    print(f"n={n:2}:  n²={n**2:>10,}    2ⁿ={2**n:>20,}")
```

:::{.notes}
"2ⁿ goes vertical."
:::

# The Practical Classes

## O(1): Constant

```{python}
#| eval: false
arr[0]
d[key]
stack.pop()
```

:::{.notes}
"No matter how big the input, same time."
:::

## O(log n): Logarithmic

```{python}
#| eval: false
def binary_search(arr, target):
    lo, hi = 0, len(arr) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            lo = mid + 1
        else:
            hi = mid - 1
    return -1
```

:::{.notes}
"Halving at each step. log₂(1 billion) ≈ 30."

We'll prove this correct next week.
:::

## O(n): Linear

```{python}
#| eval: false
def find_max(arr):
    max_val = arr[0]
    for x in arr:
        if x > max_val:
            max_val = x
    return max_val
```

:::{.notes}
"Must look at every element."
:::

## O(n²): Quadratic

```{python}
#| eval: false
def has_duplicate(arr):
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False
```

:::{.notes}
"All pairs. Gets painful quickly."
:::

## O(2ⁿ): Exponential

```{python}
#| eval: false
def all_subsets(arr):
    if not arr:
        return [[]]
    rest = all_subsets(arr[1:])
    return rest + [[arr[0]] + s for s in rest]
```

:::{.notes}
"Doubles with each element. Often intractable."
:::

# Racing Algorithms

## The Setup

```{pyodide}
import time

def has_dup_quadratic(arr):
    for i in range(len(arr)):
        for j in range(i+1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False

def has_dup_linear(arr):
    seen = set()
    for x in arr:
        if x in seen:
            return True
        seen.add(x)
    return False
```

:::{.notes}
"Same problem, two approaches."
:::

## The Race

```{pyodide}
def time_it(func, arr):
    start = time.time()
    func(arr)
    return time.time() - start

print("Size     O(n²)      O(n)")
print("-" * 35)
for n in [1000, 2000, 4000, 8000]:
    arr = list(range(n))
    t1 = time_it(has_dup_quadratic, arr)
    t2 = time_it(has_dup_linear, arr)
    print(f"{n:5}   {t1:.4f}s   {t2:.5f}s")
```

:::{.notes}
"What do you notice?"

O(n²): 4× slower when n doubles
O(n): barely changes

"The set gives O(1) lookup — transforms O(n²) into O(n)."
:::

## What Happened

O(n²) → O(n)

The **set** provides O(1) lookup.

:::{.notes}
"This is the power of data structures."
:::

# When Does It Matter?

## Small n

```{pyodide}
def slow_constant(n):
    time.sleep(0.01)  # 10ms setup
    return 42

def fast_linear(n):
    total = 0
    for i in range(n):
        total += i
    return total

for n in [10, 100, 1000]:
    t1 = time_it(slow_constant, n)
    t2 = time_it(fast_linear, n)
    print(f"n={n:4}: O(1)={t1:.3f}s  O(n)={t2:.5f}s")
```

:::{.notes}
"At small n, constants dominate."
:::

## Large n

```{pyodide}
for n in [10000, 100000, 1000000]:
    t1 = time_it(slow_constant, n)
    t2 = time_it(fast_linear, n)
    print(f"n={n:7}: O(1)={t1:.3f}s  O(n)={t2:.3f}s")
```

:::{.notes}
"At large n, Big-O dominates."
:::

## The Lesson

Small data: write clear code.

Big data: **Big-O is destiny.**

# Back to Shazam

## The Full Picture

1. Naive: O(n) — check every song
2. Smarter: O(log n) — binary search?
3. Brilliant: O(1) — hash table

:::{.notes}
"How does O(1) lookup work?"
:::

## Hash Tables

Audio → spectrogram → landmarks → **hash**

Hash → **direct lookup** in table

:::{.notes}
"Doesn't matter if 1 million or 100 million songs."

"We'll learn how in Week 5."
:::

## The Magic

| Database | Linear | Hash |
|----------|--------|------|
| 1,000 | 1 sec | 0.001 sec |
| 1,000,000 | 1000 sec | 0.001 sec |
| 100,000,000 | 27 hours | 0.001 sec |

# Week 1 Complete

## The Two Pillars

1. **Correctness** — Does it work?
2. **Efficiency** — How fast?

:::{.notes}
"An algorithm must be correct AND efficient."
:::

## What We Learned

- Counting operations, discovering patterns
- Big-O as formal notation
- The hierarchy: O(1) to O(2ⁿ)
- Data structures change everything

## Next Week

**Binary search**: the power of sorted data.

**Loop invariants**: proving correctness.

:::{.notes}
"Remember the halving pattern? Binary search IS that pattern."

"And we'll finally prove that algorithms work — not just hope."
:::

## Questions?

::: {.r-fit-text}
The right data structure

\+

The right algorithm

\=

Impossible → Instant
:::
