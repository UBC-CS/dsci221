---
title: "The Sorting Lower Bound"
subtitle: "Week 4, Wednesday"
date: 2026-01-28
---

In the Tuesday video, we saw recurrence forms and their solutions:

| Recurrence | Solution |
|------------|----------|
| $T(n) = T(n-1) + 1$ | $O(n)$ |
| $T(n) = T(n/2) + 1$ | $O(\log n)$ |
| $T(n) = 2T(n/2) + 1$ | $O(n)$ |
| $T(n) = 2T(n/2) + n$ | $O(n \log n)$ |
| $T(n) = T(n-1) + n$ | $O(n^2)$ |

Today: How do we **prove** these solutions?


## Two Methods

1. **Expansion method**: Unroll the recurrence, find the pattern, prove it

2. **Recursion tree method**: Visualize as a tree, sum the work
  - This is more of a sketch than a certainty
  - covered in enough detail in Tuesday video

Both give the same asymptotic answer. Choose whichever works for you!

# Method 1: Expansion

## The Idea

**Unroll** the recurrence step by step until you see a pattern.

Then **prove** the pattern is correct (usually by induction).

## Example 1: $T(n) = T(n-1) + c$

**Expand**:

$$T(n) = T(n-1) + c$$






## Example 2: $T(n) = T(n/2) + c$

**Expand** (assume $n = 2^k$ for simplicity):

$$T(n) = T(n/2) + c$$


## Example 3: $T(n) = 2T(n/2) + 1$

**Expand** (assume $n = 2^k$ for simplicity)::

$$T(n) = 2T(n/2) + 1$$


# A Big Question about Sorting

## Can We Do Better?

We've seen:

- Insertion sort: $O(n^2)$ (PEX1, HW1)
- Selection sort: $O(n^2)$
- Merge sort: $O(n \log n)$
- csort: $O(n)$ (HW1)

**Question**: Is $O(n \log n)$ the best we can do?

Or is there some clever algorithm that sorts in $O(n)$?

## The Surprising Answer

**Theorem**: Any comparison-based sorting algorithm requires $\Omega(n \log n)$ comparisons in the worst case.

This means:

- Merge sort is **optimal**
- No comparison sort can do better
- $O(n \log n)$ is a fundamental barrier

:::{.notes}
"This is one of the most beautiful results in CS: we can prove a lower bound on ALL possible algorithms, not just the ones we know about."
:::

# Decision Trees

## The Key Idea

Every comparison-based sort makes decisions by comparing pairs of elements:

```
if arr[i] < arr[j]:
    ...
else:
    ...
```

We can model the algorithm as a **binary tree** of decisions.

## Example: Sorting 3 Elements

Suppose we want to sort $[v_1, v_2, v_3]$.

```{=html}
<svg viewBox="0 0 600 320" xmlns="http://www.w3.org/2000/svg" style="max-width: 100%; height: auto; font-family: system-ui, sans-serif;">
  <!-- Level 0: Root -->
  <rect x="250" y="10" width="100" height="30" rx="5" fill="#e8f4fc" stroke="#3498db" stroke-width="2"/>
  <text x="300" y="30" text-anchor="middle" font-size="14" fill="#2c3e50">v₁ < v₂?</text>

  <!-- Edges from root -->
  <line x1="270" y1="40" x2="170" y2="70" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="330" y1="40" x2="430" y2="70" stroke="#7f8c8d" stroke-width="2"/>

  <!-- Level 1: Left node -->
  <rect x="120" y="70" width="100" height="30" rx="5" fill="#e8f4fc" stroke="#3498db" stroke-width="2"/>
  <text x="170" y="90" text-anchor="middle" font-size="14" fill="#2c3e50">v₂ < v₃?</text>

  <!-- Level 1: Right node -->
  <rect x="380" y="70" width="100" height="30" rx="5" fill="#e8f4fc" stroke="#3498db" stroke-width="2"/>
  <text x="430" y="90" text-anchor="middle" font-size="14" fill="#2c3e50">v₁ < v₃?</text>

  <!-- Edges from level 1 left -->
  <line x1="140" y1="100" x2="80" y2="140" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="200" y1="100" x2="260" y2="140" stroke="#7f8c8d" stroke-width="2"/>

  <!-- Edges from level 1 right -->
  <line x1="400" y1="100" x2="340" y2="140" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="460" y1="100" x2="520" y2="140" stroke="#7f8c8d" stroke-width="2"/>

  <!-- Level 2: Leaf 1 (leftmost) -->
  <rect x="30" y="140" width="100" height="30" rx="5" fill="#d5f5e3" stroke="#27ae60" stroke-width="2"/>
  <text x="80" y="160" text-anchor="middle" font-size="14" fill="#2c3e50">[__, __, __]</text>

  <!-- Level 2: Internal node 2 -->
  <rect x="210" y="140" width="100" height="30" rx="5" fill="#e8f4fc" stroke="#3498db" stroke-width="2"/>
  <text x="260" y="160" text-anchor="middle" font-size="14" fill="#2c3e50">v₁ < v₃?</text>

  <!-- Level 2: Leaf 3 -->
  <rect x="290" y="140" width="100" height="30" rx="5" fill="#d5f5e3" stroke="#27ae60" stroke-width="2"/>
  <text x="340" y="160" text-anchor="middle" font-size="14" fill="#2c3e50">[__, __, __]</text>

  <!-- Level 2: Internal node 4 -->
  <rect x="470" y="140" width="100" height="30" rx="5" fill="#e8f4fc" stroke="#3498db" stroke-width="2"/>
  <text x="520" y="160" text-anchor="middle" font-size="14" fill="#2c3e50">v₂ < v₃?</text>

  <!-- Edges from level 2 internal nodes -->
  <line x1="235" y1="170" x2="175" y2="210" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="285" y1="170" x2="345" y2="210" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="495" y1="170" x2="435" y2="210" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="545" y1="170" x2="565" y2="210" stroke="#7f8c8d" stroke-width="2"/>

  <!-- Level 3: Leaves -->
  <rect x="125" y="210" width="100" height="30" rx="5" fill="#d5f5e3" stroke="#27ae60" stroke-width="2"/>
  <text x="175" y="230" text-anchor="middle" font-size="14" fill="#2c3e50">[__, __, __]</text>

  <rect x="295" y="210" width="100" height="30" rx="5" fill="#d5f5e3" stroke="#27ae60" stroke-width="2"/>
  <text x="345" y="230" text-anchor="middle" font-size="14" fill="#2c3e50">[__, __, __]</text>

  <rect x="385" y="210" width="100" height="30" rx="5" fill="#d5f5e3" stroke="#27ae60" stroke-width="2"/>
  <text x="435" y="230" text-anchor="middle" font-size="14" fill="#2c3e50">[__, __, __]</text>

  <rect x="515" y="210" width="100" height="30" rx="5" fill="#d5f5e3" stroke="#27ae60" stroke-width="2"/>
  <text x="565" y="230" text-anchor="middle" font-size="14" fill="#2c3e50">[__, __, __]</text>
</svg>
```

## What the Tree Represents

- **Internal nodes**: Comparisons 
- **Edges**: Answers (yes/no, or $\leq$ / $>$)
- **Leaves**: sorted ordering of the elements
- **Height**: running time of the algorithm

**Note**: 
 - There must be a leaf (endpoint) for each ordering of the elements
 - the tree itself is an encoding of an algorithm

## Counting Leaves

How many different rearrangements are possible for an $n$ element list?

Each ordering needs its own leaf (otherwise we'd give the same answer for different inputs).

The decision tree must have at least \_\_\_\_\_\_ leaves!

## A Question for Later

Whose answer we need now...

A decision tree of height $h$ has at most \_\_\_\_\_\_\_ leaves?

# The Lower Bound Proof

## Connecting Height to Comparisons

Recall: The **height** of the decision tree is the **maximum number of comparisons** in the worst case.

If the tree has height $h$, then:

- At most $2^h$ leaves
- We need at least $n!$ leaves

Therefore: $2^h \geq n!$

Taking logarithms: $h \geq \log_2(n!)$

## But How Big is $\log(n!)$?

**Claim**: $\log(n!) = \Omega(n\log n)$

## The Result

**Observation**: Any comparison-based sorting algorithm requires $\Omega(n \log n)$ comparisons in the worst case.

**Justification**:

1. Model the algorithm as a decision tree
2. The tree needs at least $n!$ leaves
3. A tree with $\geq n!$ leaves has height $\geq \log_2(n!)$
4. $\log_2(n!) = \Theta(n \log n)$
5. Therefore, worst-case comparisons $\Omega(n \log n)$ ∎

:::{.notes}
"This proof is remarkable: we didn't analyze any specific algorithm. We proved a limit on ALL possible comparison-based sorting algorithms!"
:::

## What This Means

| Algorithm | Worst Case | Optimal? |
|-----------|-----------|----------|
| Merge Sort | $O(n \log n)$ | **Yes!** |
| Heap Sort | $O(n \log n)$ | **Yes!** |
| Quick Sort | $O(n^2)$ | No (but avg is optimal) |
| Insertion Sort | $O(n^2)$ | No |
| Selection Sort | $O(n^2)$ | No |

**Merge sort achieves the theoretical limit.**

# Breaking the Barrier?

## A Loophole

The lower bound applies to **comparison-based** sorts.

What if we don't compare elements?

## Counting Sort: When Elements are Small Integers

**Idea**: If elements are integers in range $[0, k]$, count occurrences!

```{pyodide}
def counting_sort(arr, k):
    """Sort integers in range [0:k]."""
    counts = [0] * k
    for x in arr:
        counts[x] += 1

    result = []
    for i in range(k):
        result.extend([i] * counts[i])
    return result
```

## Counting Sort in Action

```{pyodide}
arr = [4, 2, 0, 3, 2, 1, 4, 2]
print(f"Original: {arr}")
print(f"Sorted:   {counting_sort(arr, 4)}")
```

**Running time**: $O(n + k)$

If $k = O(n)$, this is $O(n)$—**linear time**!

## The Catch

Counting sort requires:

- Elements must be integers (or convertible to integers)
- Range $k$ must be known and reasonable
- Uses $O(k)$ extra space

**Not a general-purpose sort**—but incredibly useful when it applies!

## Other Linear-Time Sorts

| Algorithm | Constraint | Time |
|-----------|------------|------|
| Counting Sort | Integers in $[0, k]$ | $O(n + k)$ |
| Radix Sort | Fixed-width integers | $O(d \cdot n)$ |
| Bucket Sort | Uniformly distributed | $O(n)$ expected |

All exploit **extra structure** in the input.

## Python's Timsort

Python's built-in `sorted()` uses **Timsort**:

- Hybrid of merge sort and insertion sort
- Exploits "runs" of already-sorted data
- $O(n \log n)$ worst case
- $O(n)$ on nearly-sorted data
- Stable

**In practice**: Just use `sorted()`!

:::{.notes}
"Timsort is named after Tim Peters, who designed it for Python in 2002. It's now used in Java, Android, and many other systems."
:::

# Connection to This Course

## Why This Matters

1. **Lower bounds** tell us when to stop searching for better algorithms

2. **The decision tree model** appears again in:
   - Week 8: Binary search trees
   - Week 8: Wordle decision trees

3. **Divide-and-conquer analysis** (recurrences) applies to:
   - PA1: Closest pair
   - Later: Quick sort, binary search

# Summary

## What We Learned This Week

**Monday**: Merge Sort + Recurrence Forms

- Designed merge sort, saw $T(n) = 2T(n/2) + O(n)$
- Five recurrence patterns and their solutions
- Matching algorithms to recurrences

**Tuesday Video**: Solving Recurrences

- Expansion method with formal proofs
- Recursion tree method
- Proved merge sort is $O(n \log n)$

**Wednesday**: Lower Bounds

- $\Omega(n \log n)$ is optimal for comparison sorts
- Decision tree argument
- Non-comparison sorts can break the barrier

## Looking Ahead

- **PA1**: Due Feb 8 — apply divide-and-conquer to closest pair
- **EX1**: This week — covers analysis, loop invariants
- **Next week**: The dictionary trick — $O(n^2) \to O(n)$ transformations

## Questions?

:::{.notes}
Good luck on EX1!
:::
