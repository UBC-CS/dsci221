---
title: "The Hierarchy of Speed"
subtitle: "Week 2, Monday"
date: 2026-01-12
---

## Announcements

- Labs begin this week!
  - Monday 1-3pm or Tuesday 4-6pm

# Warm-up: Code Analysis

## Rules

For $T(n)$, the running time of an algorithm on input of size $n$...

1. Constants don't matter: $5n = O(n)$
2. Lower terms don't matter: $n^2 + n = O(n^2)$
3. Log base doesn't matter: $log_2 n = O(log_{10}n)$

:::{.notes}
"What's the intuition for #3?"

All logs differ by a constant factor. log₂n = log₁₀n / log₁₀2
:::

## Example 1

```{pyodide}
def computeMean(arr):
    total = 0
    for x in arr:
        total += x
    return total / len(arr)
```

| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:--:|:--:|
| 3 | 7 | 2 | 9 | 1 | 8 | 4 | 6 | 5 | 2 | 7  | 3  |

- Python questions?
- What's the worst-case running time?

:::{.notes}
One pass through the array. $\Theta(n)$, tight.

"We'll see this again later with pandas."
:::

## Example 2 {.smaller}

::: {.columns}
::: {.column width="50%"}
``` {pyodide}
n = 6
count = 0
for i in range(n):
    for j in range(n):
        count += 1
```

For $n = 6$:

- What are the values of `i` and `j` for the 17th element counted?
- What are the values of `i` and `j` for the 32nd element counted?

- What's the worst-case running time?
:::

::: {.column width="50%"}
```{python}
#| echo: false
#| fig-align: center
import matplotlib.pyplot as plt
import matplotlib.patches as patches

fig, ax = plt.subplots(figsize=(4, 4))
n = 8
for i in range(n):
    for j in range(n):
        rect = patches.Rectangle((j, n-1-i), 1, 1, linewidth=1,
                                   edgecolor='black', facecolor='#3498db')
        ax.add_patch(rect)

ax.set_xlim(0, n)
ax.set_ylim(0, n)
ax.set_aspect('equal')
ax.axis('off')
plt.tight_layout()
plt.show()
```
:::
:::

:::{.notes}
Let them trace: i=2, j=4 for 17th. i=5, j=1 for 32nd.

"Predict for n = 8?" (64)

Double n → QUADRUPLE the work. $\Theta(n^2)$, tight.
:::

## Example 3 {.smaller}

::: {.columns}
::: {.column width="50%"}

``` {pyodide}
n = 6
count = 0
for i in range(n):
    for j in range(i):
        count += 1
```

For $n = 6$:

- What are the values of `i` and `j` for the 4th element counted?
- What are the values of `i` and `j` for the 12th element counted?

- What's the worst-case running time?
:::

::: {.column width="50%"}
```{python}
#| echo: false
#| fig-align: center
import matplotlib.pyplot as plt
import matplotlib.patches as patches

fig, ax = plt.subplots(figsize=(4, 4))
n = 8
for i in range(n):
    for j in range(n):
        rect = patches.Rectangle((j, n-1-i), 1, 1, linewidth=1,
                                   edgecolor='black', facecolor='#3498db')
        ax.add_patch(rect)

ax.set_xlim(0, n)
ax.set_ylim(0, n)
ax.set_aspect('equal')
ax.axis('off')
plt.tight_layout()
plt.show()
```
:::
:::

:::{.notes}
i=2: j=0,1 (2 elements). i=3: j=0,1,2 (3 elements). So 4th is i=3, j=0.

$0 + 1 + 2 + \ldots + (n-1) = n(n-1)/2$

Still $\Theta(n^2)$, tight! The triangle is half the square.
:::


# From Last Week

## The Two Pillars

1. **Correctness** — Does it work?
2. **Efficiency** — How fast?

:::{.notes}
"Last week: efficiency. This week: we add correctness."
:::




## The Analysis Recipe

1. **Establish the case** — usually worst case
2. **Analyze the function** — count operations for that case
3. **State the bound:**
   - $\Theta$ (or "tight $O$") if you know it's exact
   - $O$ if you're only claiming an upper bound
   - $\Omega$ if you're only claiming a lower bound

:::{.notes}
"When we say 'binary search is $O(\log n)$' we mean it's *tight* — $\Theta(\log n)$. We're not hedging."

"First: which case? Then: what's the function? Then: is it tight or just a bound?"
:::

# The Hierarchy

## Big-$O$ is a Set

$O(g(n))$ is the **set** of all functions $f(n)$ satisfying the definition.

$$O(n) = \{f(n) : \exists c > 0, n_0 \geq 0 \text{ such that } f(n) \leq c \cdot n \text{ for all } n \geq n_0\}$$

:::{.notes}
"From discrete math: Big-O defines a SET of functions."
:::

## The Ladder

$$O(1) \subset O(\log n) \subset O(n) \subset O(n \log n) \subset O(n^2) \subset O(2^n)$$

Each class contains all the ones to its left.

:::{.notes}
"Every $O(1)$ function is also $O(n)$, also $O(n^2)$, etc."
:::

## A Common Abuse of Notation

We write: $\quad 4n + 2 = O(n)$

We mean: $\quad 4n + 2 \in O(n)$

:::{.notes}
"You'll see this everywhere. Just remember: we're really talking about set membership."
:::

## True or False? {.activity}

- $3n + 2 \in O(n)$
- $3n + 2 \in O(n^2)$
- $n^2 \in O(n)$
- $5 \in O(1)$
- $5 \in O(n)$

:::{.notes}
T, T, F, T, T

"If $f \in O(g)$ and $O(g) \subset O(h)$, then $f \in O(h)$."

"Let's see what these mean at scale."
:::

## What Big-$O$ Actually Tells Us

Consider $T(n) = 3n^2 + 5n + 7$. What happens when $n$ doubles?

```{pyodide}
def T(n):
    return 3*n**2 + 5*n + 7

for n in [100, 200, 400, 800, 1600]:
    print(f"n={n:4}:  T(n)={T(n):>10,}   T(2n)/T(n)={T(2*n)/T(n):.2f}")
```

:::{.notes}
The ratio approaches 4. Always. For ANY quadratic.

"The constant 3, the +5n, the +7 — none of them matter. Doubling n → quadrupling T(n). That's what $O(n^2)$ means."
:::

## The Doubling Test

| Class | When $n$ doubles, time... |
|-------|---------------------------|
| $O(1)$ | stays the same |
| $O(\log n)$ | increases by a constant |
| $O(n)$ | doubles |
| $O(n \log n)$ | slightly more than doubles |
| $O(n^2)$ | quadruples |
| $O(2^n)$ | squares (!!) |

:::{.notes}
"This is what Big-O tells us: the SHAPE of growth, not the exact numbers."
:::

# The Practical Classes

## $O(1)$: Constant

```python
arr[0]
d[key]
stack.pop()
```

"No matter how big the input, same time."

:::{.notes}
Dictionary lookup, array access, stack operations.
:::

## $O(\log n)$: Logarithmic

```python
def binary_search(arr, target):
    lo, hi = 0, len(arr) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            lo = mid + 1
        else:
            hi = mid - 1
    return -1
```

:::{.notes}
"Halving at each step. $\log_2(1 \text{ billion}) \approx 30$."

"Worst case: target not in array. We halve until the range is empty → $\Theta(\log n)$."

We'll prove this correct on Wednesday.
:::

## $O(n)$: Linear

```python
def find_max(arr):
    max_val = arr[0]
    for x in arr:
        if x > max_val:
            max_val = x
    return max_val
```

:::{.notes}
"Must look at every element once."

"Worst case = every case: we always scan the whole array. $\Theta(n)$, tight."
:::

## $O(n \log n)$: Linearithmic

```python
sorted(arr)          # Python's built-in
df.sort_values()     # pandas
```

:::{.notes}
"The sweet spot for sorting. We'll see why in Week 6."
:::

## $O(n^2)$: Quadratic

```python
def has_duplicate(arr):
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False
```

:::{.notes}
"All pairs. Gets painful quickly."

"Worst case: no duplicates → check all $n(n-1)/2$ pairs. $\Theta(n^2)$, tight."
:::

## $O(2^n)$: Exponential

```python
def all_subsets(arr):
    if not arr:
        return [[]]
    rest = all_subsets(arr[1:])
    return rest + [[arr[0]] + s for s in rest]
```

```{pyodide}
for n in [10, 20, 30, 40, 50, 100]:
    print(f"n={n:3}:  2ⁿ = {2**n:,}")
```

:::{.notes}
"Doubles with each element."

n=50: more operations than atoms in a grain of sand.
n=100: more operations than atoms in the observable universe.

At 1 billion ops/sec, n=100 would take $10^{21}$ years — heat death of universe.

"Often intractable. We'll see NP-completeness in Week 12."
:::

# Racing Algorithms

## The Setup

```{pyodide}
import time

def has_dup_quadratic(arr):
    for i in range(len(arr)):
        for j in range(i+1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False

def has_dup_linear(arr):
    seen = set()
    for x in arr:
        if x in seen:
            return True
        seen.add(x)
    return False
```

:::{.notes}
"Same problem, two approaches. Let's race them."
:::

## The Race

```{pyodide}
def time_it(func, arr):
    start = time.time()
    func(arr)
    return time.time() - start

print("Size     Θ(n²)       Θ(n)")
print("-" * 35)
for n in [1000, 2000, 4000, 8000]:
    arr = list(range(n))  # No duplicates = worst case for both
    t1 = time_it(has_dup_quadratic, arr)
    t2 = time_it(has_dup_linear, arr)
    print(f"{n:5}   {t1:.4f}s   {t2:.5f}s")
```

:::{.notes}
"We're testing worst case: no duplicates, so both algorithms scan everything."

"What do you notice?"

$\Theta(n^2)$: 4× slower when n doubles — that's quadratic!
$\Theta(n)$: barely changes — that's linear!

"The set gives $O(1)$ lookup — transforms $\Theta(n^2)$ into $\Theta(n)$."
:::

## What Happened

Worst case: $\Theta(n^2) \to \Theta(n)$

The **set** provides $O(1)$ lookup.

:::{.notes}
"This is the power of data structures. We'll learn how sets work in Week 5."

"Both bounds are tight. We're not hedging — we analyzed the worst case and know exactly how it scales."
:::

# When Does It Matter?

## The Crossover

```{pyodide}
def slow_constant(n):
    time.sleep(0.01)  # 10ms setup
    return 42

def fast_linear(n):
    total = 0
    for i in range(n):
        total += i
    return total

for n in [10, 100, 1000]:
    t1 = time_it(slow_constant, n)
    t2 = time_it(fast_linear, n)
    print(f"n={n:4}:  O(1)={t1:.3f}s   O(n)={t2:.5f}s")
```

:::{.notes}
"At small n, constants dominate. The '$O(1)$' algorithm with 10ms overhead is slower!"

Live demo: add larger n values (10000, 100000, 1000000) to show the crossover where linear catches up and passes.
:::

## The Lesson

Small data: write clear code.

Big data: **Big-O is destiny.**

## Galactic Pizza

```{pyodide}
#| echo: false
import pandas as pd
import numpy as np
import time

n = 500_000
np.random.seed(42)

df = pd.DataFrame({
    'delivery_time': np.random.exponential(scale=30, size=n),
    'planet': np.random.choice(['Earth', 'Mars', 'Europa', 'Titan'], n),
})

print(f"Rows: {len(df):,}")
print(df.head(8))
```

## Two Ways to Compute the Mean

```{pyodide}
def iter_mean(df, col):
    total = 0
    for idx, row in df.iterrows():
        total += row[col]
    return total / len(df)

start = time.time()
avg1 = iter_mean(df, 'delivery_time')
t1 = time.time() - start

start = time.time()
avg2 = df['delivery_time'].mean()
t2 = time.time() - start

print(f"Loop:       {t1:.3f} sec")
print(f"Vectorized: {t2:.5f} sec")
print(f"Speedup:    {t1/t2:,.0f}×")
```

:::{.notes}
"Both are $O(n)$. Why the difference?"
:::

## Same Big-$O$, Different Speed

Both touch every row: $O(n)$

But:

- **Vectorized**: tiny constants (compiled C)
- **Python loop**: large constants (interpreter overhead)

:::{.notes}
"Big-O tells you SCALING, not SPEED."

"In pandas: prefer vectorized operations. But that's about constants, not complexity class."
:::


# What's Next

## This Week's Arc

- **Monday** (today): The hierarchy — feel the difference
- **Tuesday video**: Loop invariants — how to prove correctness
- **Wednesday**: Binary search — the power of sorted data + proof

## The Twenty Questions Puzzle

I'm thinking of a number between 1 and 1,000,000.

You can ask yes/no questions.

What's your **guaranteed** winning strategy?

:::{.notes}
"Think about this before Wednesday."

$\log_2(1{,}000{,}000) \approx 20$

"Binary search IS the optimal strategy for this game."

"Is it your phone number? Is it your birthday? Is it 37?" — No. You need a *strategy*, not lucky guesses.
:::

## Questions?


## The Definition

f(n) is **O(g(n))** if there exist constants c > 0 and n₀ ≥ 0 such that:

$$f(n) \leq c \cdot g(n) \quad \text{for all } n \geq n_0$$

:::{.notes}
"In words: f grows no faster than g, once n is big enough."
:::

## Picture

```{python}
#| echo: false
import matplotlib.pyplot as plt
import numpy as np

n = np.linspace(1, 20, 100)
f_n = 2*n + 3
g_n = 3*n

plt.figure(figsize=(8, 5))
plt.plot(n, f_n, 'b-', linewidth=2, label='f(n) = 2n + 3')
plt.plot(n, g_n, 'r--', linewidth=2, label='c·g(n) = 3n')
plt.axvline(x=3, color='green', linestyle=':', linewidth=2, label='n₀ = 3')
plt.fill_between(n[n >= 3], f_n[n >= 3], g_n[n >= 3], alpha=0.3, color='green')
plt.xlabel('n', fontsize=12)
plt.ylabel('Operations', fontsize=12)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

:::{.notes}
For n ≥ 3: 2n + 3 ≤ 3n

Choose c = 3, n₀ = 3.
:::

## Other Notations

| Notation | Meaning |
|----------|---------|
| O(g(n)) | ≤ c·g(n) — upper bound |
| Ω(g(n)) | ≥ c·g(n) — lower bound |
| Θ(g(n)) | both — tight bound |

:::{.notes}
"We mostly use O — the worst-case guarantee."
:::

## Prove It

Show that 3n² + 5n + 2 = O(n²)

:::{.notes}
Let them try.

For n ≥ 1:
- 3n² ≤ 3n²
- 5n ≤ 5n²
- 2 ≤ 2n²

So: 3n² + 5n + 2 ≤ 10n²

Choose c = 10, n₀ = 1. ✓
:::


