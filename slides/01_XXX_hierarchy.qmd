---
title: "The Hierarchy of Speed"
subtitle: "Week 1, Wednesday"
date: 2026-01-07
---

# From Tuesday

## Patterns

| Pattern | When n doubles... |
|---------|-------------------|
| Single loop | Work doubles |
| Nested loops | Work quadruples |
| Halving | One more step |

:::{.notes}
"Today: formalize this, then FEEL the difference."
:::

# Formalizing Big-O

## The Definition

f(n) is **O(g(n))** if there exist constants c > 0 and n₀ ≥ 0 such that:

$$f(n) \leq c \cdot g(n) \quad \text{for all } n \geq n_0$$

:::{.notes}
"In words: f grows no faster than g, once n is big enough."
:::

## Picture

```{python}
#| echo: false
import matplotlib.pyplot as plt
import numpy as np

n = np.linspace(1, 20, 100)
f_n = 2*n + 3
g_n = 3*n

plt.figure(figsize=(8, 5))
plt.plot(n, f_n, 'b-', linewidth=2, label='f(n) = 2n + 3')
plt.plot(n, g_n, 'r--', linewidth=2, label='c·g(n) = 3n')
plt.axvline(x=3, color='green', linestyle=':', linewidth=2, label='n₀ = 3')
plt.fill_between(n[n >= 3], f_n[n >= 3], g_n[n >= 3], alpha=0.3, color='green')
plt.xlabel('n', fontsize=12)
plt.ylabel('Operations', fontsize=12)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

:::{.notes}
For n ≥ 3: 2n + 3 ≤ 3n

Choose c = 3, n₀ = 3.
:::

## Other Notations

| Notation | Meaning |
|----------|---------|
| O(g(n)) | ≤ c·g(n) — upper bound |
| Ω(g(n)) | ≥ c·g(n) — lower bound |
| Θ(g(n)) | both — tight bound |

:::{.notes}
"We mostly use O — the worst-case guarantee."
:::

## Prove It

Show that 3n² + 5n + 2 = O(n²)

:::{.notes}
Let them try.

For n ≥ 1:
- 3n² ≤ 3n²
- 5n ≤ 5n²
- 2 ≤ 2n²

So: 3n² + 5n + 2 ≤ 10n²

Choose c = 10, n₀ = 1. ✓
:::

## Rules

1. Constants don't matter: 5n = O(n)
2. Lower terms don't matter: n² + n = O(n²)
3. Log base doesn't matter: log₂n = O(log₁₀n)

:::{.notes}
"What's the intuition for #3?"

All logs differ by a constant factor. log₂n = log₁₀n / log₁₀2
:::

# The Hierarchy

## The Ladder

$$O(1) < O(\log n) < O(n) < O(n \log n) < O(n^2) < O(2^n)$$

:::{.notes}
"Let's see what these mean at scale."
:::

## At n = 1,000,000

| Big-O | Operations | Time @ 1B ops/sec |
|-------|------------|-------------------|
| O(1) | 1 | 1 nanosecond |
| O(log n) | 20 | 20 nanoseconds |
| O(n) | 1,000,000 | 1 millisecond |
| O(n log n) | 20,000,000 | 20 milliseconds |
| O(n²) | 10¹² | 16 minutes |
| O(2ⁿ) | 10³⁰⁰'⁰⁰⁰ | heat death of universe |

## The Cliff

```{pyodide}
for n in [10, 20, 30, 40, 50]:
    print(f"n={n:2}:  n²={n**2:>10,}    2ⁿ={2**n:>20,}")
```

:::{.notes}
"2ⁿ goes vertical."
:::

# The Practical Classes

## O(1): Constant

```{python}
#| eval: false
arr[0]
d[key]
stack.pop()
```

:::{.notes}
"No matter how big the input, same time."
:::

## O(log n): Logarithmic

```{python}
#| eval: false
def binary_search(arr, target):
    lo, hi = 0, len(arr) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            lo = mid + 1
        else:
            hi = mid - 1
    return -1
```

:::{.notes}
"Halving at each step. log₂(1 billion) ≈ 30."

We'll prove this correct next week.
:::

## O(n): Linear

```{python}
#| eval: false
def find_max(arr):
    max_val = arr[0]
    for x in arr:
        if x > max_val:
            max_val = x
    return max_val
```

:::{.notes}
"Must look at every element."
:::

## O(n²): Quadratic

```{python}
#| eval: false
def has_duplicate(arr):
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False
```

:::{.notes}
"All pairs. Gets painful quickly."
:::

## O(2ⁿ): Exponential

```{python}
#| eval: false
def all_subsets(arr):
    if not arr:
        return [[]]
    rest = all_subsets(arr[1:])
    return rest + [[arr[0]] + s for s in rest]
```

:::{.notes}
"Doubles with each element. Often intractable."
:::

# Racing Algorithms

## The Setup

```{pyodide}
import time

def has_dup_quadratic(arr):
    for i in range(len(arr)):
        for j in range(i+1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False

def has_dup_linear(arr):
    seen = set()
    for x in arr:
        if x in seen:
            return True
        seen.add(x)
    return False
```

:::{.notes}
"Same problem, two approaches."
:::

## The Race

```{pyodide}
def time_it(func, arr):
    start = time.time()
    func(arr)
    return time.time() - start

print("Size     O(n²)      O(n)")
print("-" * 35)
for n in [1000, 2000, 4000, 8000]:
    arr = list(range(n))
    t1 = time_it(has_dup_quadratic, arr)
    t2 = time_it(has_dup_linear, arr)
    print(f"{n:5}   {t1:.4f}s   {t2:.5f}s")
```

:::{.notes}
"What do you notice?"

O(n²): 4× slower when n doubles
O(n): barely changes

"The set gives O(1) lookup — transforms O(n²) into O(n)."
:::

## What Happened

O(n²) → O(n)

The **set** provides O(1) lookup.

:::{.notes}
"This is the power of data structures."
:::

# When Does It Matter?

## Small n

```{pyodide}
def slow_constant(n):
    time.sleep(0.01)  # 10ms setup
    return 42

def fast_linear(n):
    total = 0
    for i in range(n):
        total += i
    return total

for n in [10, 100, 1000]:
    t1 = time_it(slow_constant, n)
    t2 = time_it(fast_linear, n)
    print(f"n={n:4}: O(1)={t1:.3f}s  O(n)={t2:.5f}s")
```

:::{.notes}
"At small n, constants dominate."
:::

## Large n

```{pyodide}
for n in [10000, 100000, 1000000]:
    t1 = time_it(slow_constant, n)
    t2 = time_it(fast_linear, n)
    print(f"n={n:7}: O(1)={t1:.3f}s  O(n)={t2:.3f}s")
```

:::{.notes}
"At large n, Big-O dominates."
:::

## The Lesson

Small data: write clear code.

Big data: **Big-O is destiny.**

# Back to Shazam

## The Full Picture

1. Naive: O(n) — check every song
2. Smarter: O(log n) — binary search?
3. Brilliant: O(1) — hash table

:::{.notes}
"How does O(1) lookup work?"
:::

## Hash Tables

Audio → spectrogram → landmarks → **hash**

Hash → **direct lookup** in table

:::{.notes}
"Doesn't matter if 1 million or 100 million songs."

"We'll learn how in Week 5."
:::

## The Magic

| Database | Linear | Hash |
|----------|--------|------|
| 1,000 | 1 sec | 0.001 sec |
| 1,000,000 | 1000 sec | 0.001 sec |
| 100,000,000 | 27 hours | 0.001 sec |

# Week 1 Complete

## The Two Pillars

1. **Correctness** — Does it work?
2. **Efficiency** — How fast?

:::{.notes}
"An algorithm must be correct AND efficient."
:::

## What We Learned

- Counting operations, discovering patterns
- Big-O as formal notation
- The hierarchy: O(1) to O(2ⁿ)
- Data structures change everything

## Next Week

**Binary search**: the power of sorted data.

**Loop invariants**: proving correctness.

:::{.notes}
"Remember the halving pattern? Binary search IS that pattern."

"And we'll finally prove that algorithms work — not just hope."
:::

## Questions?

::: {.r-fit-text}
The right data structure

\+

The right algorithm

\=

Impossible → Instant
:::


<!-- extra slides to fit into narrative:

## Example: Nested Loops

``` {pyodide}
count = 0
for i in range(n):
    for j in range(n):
        count += 1
```

Trace for n = 4. How many increments?

:::{.notes}
Let them trace: 16

"Predict for n = 8?" (64)

Double n → QUADRUPLE the work. QUADRATIC.
:::

## Nested Loops (variant)

```{python}
#| eval: false
count = 0
for i in range(n):
    for j in range(i):
        count += 1
```

Trace for n = 5.

:::{.notes}
i=0: 0. i=1: 1. i=2: 2. i=3: 3. i=4: 4. Total: 10.

0 + 1 + 2 + ... + (n-1) = n(n-1)/2

Still quadratic!
:::

## The Triangle Sum

$$1 + 2 + 3 + \ldots + n = \frac{n(n+1)}{2}$$

:::{.notes}
"This is O(n²), not O(n)."

Whenever you see "all pairs" — expect quadratic.
:::

## The Halving Pattern

```{python}
#| eval: false
count = 0
i = n
while i > 1:
    count += 1
    i = i // 2
```

Trace for n = 1000.

:::{.notes}
1000 → 500 → 250 → 125 → 62 → 31 → 15 → 7 → 3 → 1

About 10 iterations. log₂(1000) ≈ 10.

"What if n = 1,000,000?" (≈20)

Double n → ONE more step. LOGARITHMIC.
:::

## Let's verify

```{pyodide}
import math

n = 1000
i = n
count = 0
while i > 1:
    print(f"i = {i}")
    i = i // 2
    count += 1

print(f"\nIterations: {count}")
print(f"log₂({n}) = {math.log2(n):.1f}")
```

## Pattern Summary

| Pattern | What happens when n doubles? | Name |
|---------|------------------------------|------|
| Single loop | Work doubles | O(n) |
| Nested loops | Work quadruples | O(n²) |
| Halving | One more step | O(log n) |

:::{.notes}
"These are the building blocks."
:::

## Big-O Notation {.smaller}

::: {.columns}
::: {.column width="55%"}

| Notation | Name |
|----------|------|
| O(1) | Constant |
| O(log n) | Logarithmic |
| O(n) | Linear |
| O(n log n) | Linearithmic |
| O(n²) | Quadratic |
| O(2ⁿ) | Exponential |

:::

::: {.column width="45%"}
```{python}
#| echo: false
import matplotlib.pyplot as plt
import numpy as np

n = np.linspace(1, 15, 100)
const = np.ones_like(n)
log_n = np.log2(n) * 2
linear = n * 1.5
n_log_n = n * np.log2(n) * 0.8
quadratic = n ** 2 / 4
exponential = 2 ** (n / 2.5)

plt.figure(figsize=(5, 4))
plt.plot(n, const, '-', linewidth=2.5, label='O(1)')
plt.plot(n, log_n, '-', linewidth=2.5, label='O(log n)')
plt.plot(n, linear, '-', linewidth=2.5, label='O(n)')
plt.plot(n, n_log_n, '-', linewidth=2.5, label='O(n log n)')
plt.plot(n, quadratic, '-', linewidth=2.5, label='O(n²)')
plt.plot(n, exponential, '-', linewidth=2.5, label='O(2ⁿ)')
plt.xlabel('n')
plt.ylabel('Time')
plt.ylim(0, 70)
plt.xlim(1, 15)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```
:::
:::

:::{.notes}
"We'll formalize this Wednesday."
:::

# DataFrame Operations

## Which pattern?

```{python}
#| eval: false
df['col'].sum()
df[df['col'] > 5]
df.loc[12345]
df.merge(other_df)
```

:::{.notes}
Let them reason:

- sum: touch every value → O(n)
- filter: check every row → O(n)
- loc: direct access → O(1)
- merge: ???

"Is merge O(n²)? It compares rows..."
:::

## The Merge Mystery

`df.merge()` uses hashing internally.

O(n), not O(n²).

:::{.notes}
"We'll learn how in Week 5."
:::

## Galactic Pizza

```{pyodide}
#| echo: false
import pandas as pd
import numpy as np
import time

n = 500_000
np.random.seed(42)

df = pd.DataFrame({
    'delivery_time': np.random.exponential(scale=30, size=n),
    'planet': np.random.choice(['Earth', 'Mars', 'Europa', 'Titan'], n),
})

print(f"Rows: {len(df):,}")
print(df.head(8))
```

## Two ways to compute the mean

```{pyodide}
def iter_mean(df, col):
    total = 0
    for idx, row in df.iterrows():
        total += row[col]
    return total / len(df)

start = time.time()
avg1 = iter_mean(df, 'delivery_time')
t1 = time.time() - start

start = time.time()
avg2 = df['delivery_time'].mean()
t2 = time.time() - start

print(f"Loop:       {t1:.3f} sec")
print(f"Vectorized: {t2:.5f} sec")
print(f"Speedup:    {t1/t2:,.0f}x")
```

:::{.notes}
"Both are O(n). Why the difference?"
:::

## Same Big-O, Different Speed

Both touch every row: O(n)

But:

- **Vectorized**: tiny constants (compiled C)
- **Python loop**: large constants (interpreter overhead)

:::{.notes}
"Big-O tells you SCALING, not SPEED."
:::

# Summary

## What we learned

1. Trace code to count operations
2. Cases: best, worst, average
3. Patterns: linear, quadratic, logarithmic
4. Big-O describes the shape

## Wednesday

The full hierarchy. Racing algorithms. Feeling the difference.

:::{.notes}
"And we'll finally see how Shazam's O(1) lookup works."
:::


<!-- from Monday: 


## Constants Don't Matter

T(n) = 2n + 1?

:::{.notes}
"Does the 2 matter? Does the +1 matter?"

At n = 100,000,000, the constants are noise.

What matters is the SHAPE: linear growth.

"We write this as O(n)."
:::

## Growth Rates {.smaller}

::: {.columns}
::: {.column width="45%"}
**Algorithm A**

| n | time |
|---|------|
| 10 | 1 sec |
| 50 | 5 sec |
| 100 | 10 sec |

**Algorithm B**

| n | time |
|---|------|
| 10 | 1 sec |
| 50 | 25 sec |
| 100 | 100 sec |
:::

::: {.column width="55%"}
```{python}
#| echo: false
import matplotlib.pyplot as plt
import numpy as np

n = np.linspace(1, 100, 100)
algo_a = n * 0.1
algo_b = (n ** 2) * 0.01

plt.figure(figsize=(5, 4))
plt.plot(n, algo_a, 'b-', linewidth=2, label='Algorithm A')
plt.plot(n, algo_b, 'r-', linewidth=2, label='Algorithm B')
plt.xlabel('Input size (n)')
plt.ylabel('Time (seconds)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```
:::
:::

:::{.notes}
"What's the pattern for A? For B?"

A: linear (time ∝ n)
B: quadratic (time ∝ n²)
:::

## What about pandas?

You'll work with DataFrames that have **millions of rows**.

Some operations are fast. Others are slow.

:::{.notes}
"Why?"

- `df['col'].sum()` — must touch every value
- `df.loc[12345]` — goes directly to one row
- `df.iterrows()` — visits every row, with Python overhead

"We'll learn to predict which is which."
:::

# Back to Shazam


-->